[{"authors":["xiaoou"],"categories":null,"content":"Thème\nSur fond de tension entre la Chine et les Etats-Unis,\nnous nous sommes penchés sur la représentation de la guerre commerciale sur la toile des pays anglophones, de la Chine, et de la France.\nMotivation\nLa guerre commerciale est susceptible de causer une récession générale.\nLes Etats-Unis semblent considérer que la montée de la Chine menace déjà son hégémonie, alors que la Chine, aspirant à des progrès économiques toujours plus importants, prend la défense de ses propres intérêts.\nExclusivités\n Syntaxe highlighting  Tous les codes sont colorés pour faciliter la lecture, que cela soit Python, Javascript, Perl ou Shell (bash).\n Recherche productible  Nos codes sont écrits dans des fichiers .Rmd. Il suffit de les ouvrir dans Rstudio pour les exécuter et voir ainsi les résultats.\n Git  L\u0026rsquo;historique de notre projet est intégralement conservée et documentée grâce à Git, logiciel de gestion de versions décentralisé. Les codes Rmarkdown se trouvent ici et les codes du site ici.\n Responsive Design  Notre site s\u0026rsquo;adapte automatiquement à différentes tailles d\u0026rsquo;écran.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"51b9199d1441fa523628dd4669da1c42","permalink":"/projet_encadre/authors/xiaoou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projet_encadre/authors/xiaoou/","section":"authors","summary":"Thème\nSur fond de tension entre la Chine et les Etats-Unis,\nnous nous sommes penchés sur la représentation de la guerre commerciale sur la toile des pays anglophones, de la Chine, et de la France.\nMotivation\nLa guerre commerciale est susceptible de causer une récession générale.\nLes Etats-Unis semblent considérer que la montée de la Chine menace déjà son hégémonie, alors que la Chine, aspirant à des progrès économiques toujours plus importants, prend la défense de ses propres intérêts.","tags":null,"title":"ô la guerre, quand tu nous tiens !","type":"authors"},{"authors":null,"categories":null,"content":" Navigation A gauche vous trouvez la liste des billets écrits après chaque cours\nA droite vous avez accès au sommaire de chaque billet.\nPour vous faciliter la navigation, nous avons également mis à disposition la liste des billets sur cette page.\nStructuration des billets N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nListe des billets  cours 2 - Préambule Unix I cours 3 - Préambule Unix II cours 4 - Préparation du projet cours 6 - Détection de l’encodage d’URL cours 7 - Rajouter 4 colonnes dans la table d\u0026rsquo;URLs cours 8 - Résolution du problème d\u0026rsquo;encodage cours 9 - Deuxième méthode pour la segmentation du corpus chinois cours 10 - Reconstruction du code (Refactoring)  ","date":1578178800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1578178800,"objectID":"b0d8ccffb8d530bff08365a148a04ee6","permalink":"/projet_encadre/blog/","publishdate":"2020-01-05T00:00:00+01:00","relpermalink":"/projet_encadre/blog/","section":"blog","summary":" Navigation A gauche vous trouvez la liste des billets écrits après chaque cours\nA droite vous avez accès au sommaire de chaque billet.\nPour vous faciliter la navigation, nous avons également mis à disposition la liste des billets sur cette page.\nStructuration des billets N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nListe des billets  cours 2 - Préambule Unix I cours 3 - Préambule Unix II cours 4 - Préparation du projet cours 6 - Détection de l’encodage d’URL cours 7 - Rajouter 4 colonnes dans la table d\u0026rsquo;URLs cours 8 - Résolution du problème d\u0026rsquo;encodage cours 9 - Deuxième méthode pour la segmentation du corpus chinois cours 10 - Reconstruction du code (Refactoring)  ","tags":null,"title":"Mode d'emploi","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nSystème de fichiers Sous windows, il y a plusieurs arborescences tandis que sous unix, on a qu’une arborescence.\n# print repertoire actuel pwd # remonte au repertoire parent cd .. # aller a la racine cd / echo \u0026quot;# exemple de l'arborescence de la racine sous Mac\u0026quot; # exemple de l'arborescence de la racine sous Mac ls # aller a home cd ~ echo \u0026quot;# sortie de la commande cat\u0026quot; echo \u0026quot;Bonjour\u0026quot; \u0026gt; test.txt # visualiser le contenu d'un fichier cat test.txt echo \u0026quot;# montrer les metadonnes du contenu\u0026quot; # la commande egale wc -mlw, m = caractere l = ligne, w = mots wc test.txt  /Users/becca/Downloads/siteProjetEncadre/content/blog # exemple de l'arborescence de la racine sous Mac Applications Developer Incompatible Software Library Network System Users Volumes anaconda3 bin cores dev etc home installer.failurerequests net opt private sbin tmp usr var # sortie de la commande cat Bonjour # montrer les metadonnes du contenu 1 1 8 test.txt  ","date":1569967200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569967200,"objectID":"6e16f9795c5b0f83798e56ef0528f59c","permalink":"/projet_encadre/blog/1/","publishdate":"2019-10-02T00:00:00+02:00","relpermalink":"/projet_encadre/blog/1/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nSystème de fichiers Sous windows, il y a plusieurs arborescences tandis que sous unix, on a qu’une arborescence.\n# print repertoire actuel pwd # remonte au repertoire parent cd .. # aller a la racine cd / echo \u0026quot;# exemple de l'arborescence de la racine sous Mac\u0026quot; # exemple de l'arborescence de la racine sous Mac ls # aller a home cd ~ echo \u0026quot;# sortie de la commande cat\u0026quot; echo \u0026quot;Bonjour\u0026quot; \u0026gt; test.","tags":null,"title":"Cours 2 - Préambule Unix I","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nFlux d\u0026rsquo;entrée \u0026amp; de sortie \u0026amp; d\u0026rsquo;erreur # flux d’erreur standard \u0026quot;\u0026gt;\u0026gt;\u0026quot; (sortie normal) vs. 2\u0026gt; (canal d’erreur) cd ~ lsd \u0026gt; sortie.txt 2\u0026gt; erreur.txt # exercice : transformer tous les n en N d'un fichier et l'enregistrer dans un autre fichier tr \u0026quot;n\u0026quot; \u0026quot;N\u0026quot; \u0026lt; test.txt \u0026gt; testN.txt cat testN.txt # Exercice : majusculiser les données tr \u0026quot;[[:lower:]]\u0026quot; \u0026quot;[[:upper:]]\u0026quot; \u0026lt; test.txt  BoNjour BONJOUR  Redirection du flux d’information Récupérer l’output de la première commande et le réenvoyer à la deuxième commande - symbole clé : pipe |\n# transformer \u0026quot;é\u0026quot; en \u0026quot;E\u0026quot; puis majusculiser l'output de la première commande cd ~ tr \u0026quot;é\u0026quot; \u0026quot;E\u0026quot; \u0026lt; test.txt | tr \u0026quot;[[:lower:]]\u0026quot; \u0026quot;[[:upper:]]\u0026quot; # trier par le premier champ - sort ; filtrage de doublons - uniq egrep -o \u0026quot;\\w+\u0026quot; test.txt | sort | uniq -c  BONJOUR 1 Bonjour  Sectionner les données par champ # couper les données en 2 champs par le délimiteur \u0026quot;=\u0026quot; cd ~ echo \u0026quot;la somme de 2+2=4\u0026quot; \u0026gt; test2.txt cut -f2 -d\u0026quot;=\u0026quot; test2.txt # utiliser read pour capturer la saisie de l'utilisateur et la stocker dans une variable echo \u0026quot;nom ?\u0026quot; ; # read nom; echo \u0026quot;bonjour $nom\u0026quot;  4 nom ? bonjour  ","date":1569967200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569967200,"objectID":"23189ba8a1714c94f6dfaf5b36cdaffa","permalink":"/projet_encadre/blog/2/","publishdate":"2019-10-02T00:00:00+02:00","relpermalink":"/projet_encadre/blog/2/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nFlux d\u0026rsquo;entrée \u0026amp; de sortie \u0026amp; d\u0026rsquo;erreur # flux d’erreur standard \u0026quot;\u0026gt;\u0026gt;\u0026quot; (sortie normal) vs. 2\u0026gt; (canal d’erreur) cd ~ lsd \u0026gt; sortie.txt 2\u0026gt; erreur.txt # exercice : transformer tous les n en N d'un fichier et l'enregistrer dans un autre fichier tr \u0026quot;n\u0026quot; \u0026quot;N\u0026quot; \u0026lt; test.txt \u0026gt; testN.txt cat testN.txt # Exercice : majusculiser les données tr \u0026quot;[[:lower:]]\u0026quot; \u0026quot;[[:upper:]]\u0026quot; \u0026lt; test.","tags":null,"title":"Cours 3 - Préambule Unix II","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nConfiguration du terrain  Nous avons créé l\u0026rsquo;arborescence de travail à l\u0026rsquo;aide du script bash suivant nommé prepare-environnement-projet.sh\n#!/bin/bash mkdir PROJET_MOT_SUR_LE_WEB cd PROJET_MOT_SUR_LE_WEB # on peut désormais créer l'arborescence de travail mkdir ./CONTEXTES; mkdir ./DUMP_TEXT; mkdir ./IMAGES; mkdir ./PAGES_ASPIREES; mkdir ./PROGRAMMES; mkdir ./TABLEAUX; mkdir ./URLS; # les lignes précédentes peuvent tenir sur une seule ligne # à savoir mkdir ./CONTEXTES ./DUMP_TEXT etc.... # le dossier URLS contiendra le fichier initial d'URLs  Il suffit de nous positionner dans le même répertoire que ce script et l\u0026rsquo;exécuter avec bash\nbash prepare-environnement-projet.sh   Lecture des fichiers URL et écriture de leurs contenus dans un nouveau fichier J\u0026rsquo;ai une remarque sur la manière de récupérer tous les fichiers dans un répertoire. J\u0026rsquo;ai utilisé /chemin/* au lieu de $(ls /chemin) car ce dernier m\u0026rsquo;apparaît un peu lourd mais je vois pas leur différence au niveau de sortie\n# !/bin/bash # on commence par effacer l'éventuel contenu de ficher que l'on doit réécrire echo \u0026quot;\u0026quot; \u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # on récupère les 2 arguments que l'on a passé au programme # le premier : chemin vers le dossier contenant les fichiers d'URL # le second : chemin vers le dossier devant contenir le fichier HTML final echo \u0026quot;les urls sont dans : $1\u0026quot;; echo \u0026quot;chemin de stockage : $2\u0026quot;; # pour tous les fichiers dans le répertoire 1 for fichier in $1/* # on exécute les commandes suivantes do # compteur destiné à compter les URLs pour chaque fichier d'URL compteur=1; echo \u0026quot;$fichier\u0026quot;; for ligne in $(cat \u0026quot;$fichier\u0026quot;) do echo \u0026quot;$compteur : $ligne\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # on incrémente le compteur des URLs compteur=$((compteur+1)) done done  ","date":1570572000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570572000,"objectID":"f330f6efe29e9f6d880fd39a21d463e9","permalink":"/projet_encadre/blog/3/","publishdate":"2019-10-09T00:00:00+02:00","relpermalink":"/projet_encadre/blog/3/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nConfiguration du terrain  Nous avons créé l\u0026rsquo;arborescence de travail à l\u0026rsquo;aide du script bash suivant nommé prepare-environnement-projet.sh\n#!/bin/bash mkdir PROJET_MOT_SUR_LE_WEB cd PROJET_MOT_SUR_LE_WEB # on peut désormais créer l'arborescence de travail mkdir ./CONTEXTES; mkdir ./DUMP_TEXT; mkdir ./IMAGES; mkdir ./PAGES_ASPIREES; mkdir ./PROGRAMMES; mkdir ./TABLEAUX; mkdir ./URLS; # les lignes précédentes peuvent tenir sur une seule ligne # à savoir mkdir .","tags":null,"title":"Cours 4 - Préparation du projet","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nRécapitulatif des trois dernières séances  Nous avons établi d’abord des tables de base contenant des URLs à l’aide de deux boucles for, dont l\u0026rsquo;une pour parcourir tous nos fichiers d’URLs et construire une table pour chaque fichier, soit chaque langue et la deuxième servant à lire les URLs et les écrire dans nos tables ligne à ligne. Nous avons appris que le shell utilise des variables d’environnement dont l’on peut faire apparaître le contenu avec le signe dollar “$”. En utilisant ce signe, nous pouvons stocker et réutiliser le numéro de ligne et le contenu correspondant.\n Ayant pour but d\u0026rsquo;aspirer le contenu de page web et d\u0026rsquo;en faire des analyses textuelles, nous avons utilisé les commandes curl et lynx. Deux nouvelles colonnes ont été rajoutées pour enregistrer les liens locaux de fichier html et txt dirigés vers le contenu sans format et celui de texte brut de page web.\n Ce processus n\u0026rsquo;est pas automatique à cause des problèmes d’URLs ou de l’encodage. Pour pallier ces problèmes, nous avons détecté si le code de réponse est 200 qui veut dire requête réussie et si l’encodage est UTF-8. Les traitements ultérieurs dépendent des résultats de ces vérifications. Ces deux informations ont également été rajoutées dans nos tables.\n  Problèmes rencontrés Nous n’avons pas pris conscience de l’influence d’espace dans le script bash jusqu’au moment de l’apparition des erreurs suivantes :\n[200: command not found # ou [405: command not found  [200==200]: command not found  La première erreur provient de\nif [ $coderetour == 200 ]  Nous avons utilisé les crochets simples et mis des espaces ente $coderetour et 200.\nLa deuxième erreur est à cause de\nif [$coderetour==200]  ce qui n\u0026rsquo;obéit pas à la syntaxe du bash dans laquelle la condition est toujours entourée d’un espace après le crochet d’ouverture et avant le crochet de fermeture.\nNous avons aussi essayé de comprendre la différence entre les crochets simples et doubles.\nLes conditions à crochets doubles permettent tout ce qu’offrent les conditions à crochets simples. En outre, elles acceptent l’usage du wildcard comme en bash ainsi que des expressions régulières.\nVoici la dernière version de script accompagnée des explications de certaines commandes bash dans les commentaires.\n# !/bin/bash # on commence par effacer l'éventuel contenu de ficher que l'on doit réécrire echo \u0026quot;\u0026quot; \u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # ou # rm -f \u0026quot;$2/tableau.html\u0026quot; ; # on récupère les 2 arguments que l'on a passés au programme # le premier : chemin vers le dossier contenant les fichiers d'URL # le second : chemin vers le dossier devant contenir le fichier HTML final echo \u0026quot;les urls sont dans : $1\u0026quot;; echo \u0026quot;chemin de stockage : $2\u0026quot;; # 1. définir le type \u0026lt;html\u0026gt; echo \u0026quot;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta charset=\\\u0026quot;UTF-8\\\u0026quot;\u0026gt;\u0026lt;title\u0026gt;Tableaux\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # 2. générer un tableau par fichier d'URL numeroTable=1; # pour tous les fichiers dans le répertoire 1 # for fichier in $1/* for fichier in $(ls $1) # on exécute les commandes suivantes do # compteur destiné à compter les URLs pour chaque fichier d'URL compteur=1; echo \u0026quot;$1/$fichier\u0026quot;; echo \u0026quot;\u0026lt;table border=\\\u0026quot;2\\\u0026quot; align=\\\u0026quot;center\\\u0026quot; width=\\\u0026quot;80%\\\u0026quot;\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # lecture ligne à ligne des URLs for ligne in $(cat \u0026quot;$1/$fichier\u0026quot;) do # curl ： un outil de transfert de data de ou vers un serveur # options : # -o output file # -s silent, use -S, --show-error in addition to this option to disable progress meter but still show error messages # -I head, fetch the headers only # -L location, if the server reports that the requested page has moved to a different location, this option will make curl redo the request on the new place. # -w write-out, Make curl display information on stdout after a completed transfer coderetour=$(curl -SIL -o tmp.txt -w %{http_code} $ligne); # si coderetour égal à 200 if [[ $coderetour == 200 ]] then # normaliser la case, tout est en majuscule, comme UTF-8 # supprimer eventuellement la fin de ligne encodage=$(curl -sIL -o toto -w %{content_type} $ligne|cut -f2 -d\u0026quot;=\u0026quot;|tr '[a-z]' '[A-Z]'|tr -d '\\n'); # quand on ouvre un fichier, le reprtoire sera le sien # pour relier $numeroTable et $compteur, '-' marche mais pas '_'????? curl -L -o \u0026quot;../PAGES-ASPIREES/$numeroTable-$compteur.html\u0026quot; \u0026quot;$ligne\u0026quot;; if [[ $encodage == 'UTF-8' ]] then lynx -dump -nolist -assume-charset=$encodage - display-charset=$encodage \u0026quot;../PAGE_ASPIREES/$numeroTable-$compteur.html\u0026quot; \u0026gt; ../DUMP-TEXT/$numeroTable-$compteur.txt; echo \u0026quot;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;$compteur\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;$ligne\\\u0026quot; target=\\\u0026quot;_blank\\\u0026quot;\u0026gt;$ligne\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Code_http:$coderetour\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Encodage:$encodage\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../PAGE-ASPIREES/$numeroTable-$compteur.html\\\u0026quot;\u0026gt;$numeroTable-$compteur.html\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../DUMP-TEXT/$numeroTable-$compteur.txt\\\u0026quot;\u0026gt;$numeroTable-$compteur.txt\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; else # comment savoir si la valeur est connu de iconv # extraction de l'encodage avec egrep dans la page aspiree # lynx dump avec l'encodage trouve # convertir le dump en utf8 avec iconv echo \u0026quot;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;$compteur\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;$ligne\\\u0026quot; target=\\\u0026quot;_blank\\\u0026quot;\u0026gt;$ligne\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Code_http:$coderetour\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Encodage:$encodage\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../PAGE_ASPIREES/$numeroTable-$compteur.html\\\u0026quot;\u0026gt;\u0026quot;$numeroTable-$compteur.html\u0026quot;\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; fi else echo \u0026quot;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;$compteur\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;$ligne\\\u0026quot; target=\\\u0026quot;_blank\\\u0026quot;\u0026gt;$ligne\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Code_http:$coderetour\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; fi compteur=$((compteur+1)); done echo \u0026quot;\u0026lt;/table\u0026gt;\u0026lt;br /\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;; # cat $fichier \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot; numeroTable=$((numeroTable+1)); done echo \u0026quot;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$2/tableau.html\u0026quot;;  ","date":1571781600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571781600,"objectID":"d25ee74feef451d187a06b12d7e25e7e","permalink":"/projet_encadre/blog/4/","publishdate":"2019-10-23T00:00:00+02:00","relpermalink":"/projet_encadre/blog/4/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nRécapitulatif des trois dernières séances  Nous avons établi d’abord des tables de base contenant des URLs à l’aide de deux boucles for, dont l\u0026rsquo;une pour parcourir tous nos fichiers d’URLs et construire une table pour chaque fichier, soit chaque langue et la deuxième servant à lire les URLs et les écrire dans nos tables ligne à ligne.","tags":null,"title":"Cours 6 - Détection de l’encodage d’URL","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nArguments Désormais, au moment du lancement de notre programme, 3 arguments sont requis. Le premier et le deuxième sont le répertoire de l\u0026rsquo;input URL et celui de l\u0026rsquo;output tableau. Le troisième est nos mots ciblés dans notre projet qui est donc \u0026lsquo;guerre commerciale|trade war|贸易战\u0026rsquo;. Ce troisième argument sert à compter sa fréquence dans le texte.\nColonnes rajoutées Les 4 colonnes rajoutées sont :\n le contexte de mot-clé sa fréquence dans chaque fichier dump l\u0026rsquo;index des mots présents dans chaque fichier dump les bigrammes  Minigrep ou egrep Après avoir intallé Perl, nous avons deux façons de trouver le contexte des mots ciblés, soit deux lignes autour de la ligne concernant les mots-clés, une avec egrep et une autre avec minigrep\n# ----06/11 # METHODE 1 : trouver la ligne avant et la ligne après de la ligne contenant notre mot-clé avec egrep # egrep -i -C 2 $motif '../DUMP-TEXT/$numerotableau-$compteur.txt' \u0026gt; '../CONTEXTES/$numerotableau-$compteur.txt'; # METHODE 2 : en utilisant minigrep # mettre les motifs dans le fichier parametre-motif.txt minigrep/minigrepmultilingue.pl \u0026quot;UTF-8\u0026quot; '../DUMP-TEXT/$numerotableau-$compteur.txt' minigrep/parametre-motif.txt; # renommer le fichier obtenu mv resultat-extraction.html ../CONTEXTES/$numerotableau-$compteur.html  Afin d\u0026rsquo;obtenir des bigrammes, nous avons concaténé tous les mots d\u0026rsquo;un fichier avec tous les mots du même fichier excepté le premier en utilisant paste. Cependant, cette manière de récupérer des mots ne s\u0026rsquo;applique pas au chinois à cause du fait que le chinois n\u0026rsquo;utilise pas des délimiteurs pour distinguer des mots. Il nous faut donc trouver un autre outil pour la segmentation du corpus chinois.\n# \\w+ ne marche que dans UTF-8 # INDEX DES MOTS egrep -o '\\w+' '../DUMP-TEXT/$numerotableau-$compteur.txt' | sort | uniq -c|sort -gr \u0026gt; '../DUMP-TEXT/index-$numerotableau-$compteur.txt'; ## BIGRAMS egrep -o '\\w+' '../DUMP-TEXT/$numerotableau-$compteur.txt' \u0026gt;fic1; tail -n +2 fich1 \u0026gt;fic2; paste fic1 fic2 | sort | uniq -c | sort -gr \u0026gt; '../DUMP-TEXT/bigrams-$numerotableau-$compteur.txt' ## COMPTER LE MOTIF frqMotif=$(egrep -oc $motif '../DUMP-TEXT/$numerotableau-$compteur.txt');  Solution pour la segmentation du corpus chinois Grâce aux blogs de nos anciens camarades, nous avons pris connaissance d\u0026rsquo;un outil de segmentation du chinois appelé Stanford Chinese Segmenter. Le site officiel offre des explications très détaillées sur l\u0026rsquo;usage de cet outil.\nsh ./stanford-segmenter/segment.sh -k ctb ./DUMP-TEXT/*.txt utf-8 0  ","date":1572994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572994800,"objectID":"98258f7ceffb451ca7b1d447f01f95c1","permalink":"/projet_encadre/blog/5/","publishdate":"2019-11-06T00:00:00+01:00","relpermalink":"/projet_encadre/blog/5/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nArguments Désormais, au moment du lancement de notre programme, 3 arguments sont requis. Le premier et le deuxième sont le répertoire de l\u0026rsquo;input URL et celui de l\u0026rsquo;output tableau. Le troisième est nos mots ciblés dans notre projet qui est donc \u0026lsquo;guerre commerciale|trade war|贸易战\u0026rsquo;. Ce troisième argument sert à compter sa fréquence dans le texte.","tags":null,"title":"Cours 7 - Rajouter 4 colonnes dans la table d'URLs","type":"docs"},{"authors":null,"categories":null,"content":"Lors de la détection de l\u0026rsquo;encodage, pour quelques pages chinois, nous avons eu TEXT/HTML comme retour qui n\u0026rsquo;indique aucun encodage et par conséquent rend difficile la conversion en UTF-8 avec iconv. Il nous faut donc reconstruire nos codes selon le processus ci-après :\n","date":1572994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572994800,"objectID":"78d8af5e42305039d3b1e34d571c3031","permalink":"/projet_encadre/blog/6/","publishdate":"2019-11-06T00:00:00+01:00","relpermalink":"/projet_encadre/blog/6/","section":"blog","summary":"Lors de la détection de l\u0026rsquo;encodage, pour quelques pages chinois, nous avons eu TEXT/HTML comme retour qui n\u0026rsquo;indique aucun encodage et par conséquent rend difficile la conversion en UTF-8 avec iconv. Il nous faut donc reconstruire nos codes selon le processus ci-après :","tags":null,"title":"Cours 8 - Résolution du problème d'encodage","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nSegmentation du corpus chinois Pour la segmentation du corpus chinois, nous avions précédemment utilisé le stanford-segmenter. Pour comparer sa performance avec un module Python Jieba spécialisé dans la segmentation du texte chinois, nous avons écrit un script python en important ce dernier module.\n#!/usr/bin/python # _*_ coding: utf-8 _*_ # segment chinese text # modules import re import sys import jieba # functions def tokenize(file): # input file with open(file, 'r', encoding='utf-8') as f: content = f.read() # clean text and keep only chinese characters pattern=re.compile(u'[^\\u4E00-\\u9FA5]') texte=pattern.sub(r'', content) wordlist_temp=list(jieba.cut(texte, cut_all=False)) wordlist=[i.rstrip() for i in wordlist_temp if len(i)\u0026gt;=1] return wordlist def token_file(file): wordlist=tokenize(sys.argv[1]) # output file with open(file, 'w', encoding='utf-8') as f: f.write(' '.join(wordlist)) if __name__ == \u0026quot;__main__\u0026quot;: token_file(sys.argv[2])  Il suffit de taper le nom du fichier contenant des textes chinois comme le premier argument et le nom du fichier de sortie comme le deuxième argument.\npython3.7 seg_jieba.py ../DUMP-TEXT/1-16.txt test1.txt  Nous avons ensuite fait un test en prenant un de nos dump textes comme l\u0026rsquo;input avec les segmenteurs Jieba et Stanford-Segmenter. Les résultats sont affichés ci-après. Le résultat de Jieba s\u0026rsquo;affiche en haut tandis que celui de Stanford se trouve en bas.\n","date":1572994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572994800,"objectID":"2f9425d6b16a5b978d42c59ea483741d","permalink":"/projet_encadre/blog/7/","publishdate":"2019-11-06T00:00:00+01:00","relpermalink":"/projet_encadre/blog/7/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nSegmentation du corpus chinois Pour la segmentation du corpus chinois, nous avions précédemment utilisé le stanford-segmenter. Pour comparer sa performance avec un module Python Jieba spécialisé dans la segmentation du texte chinois, nous avons écrit un script python en important ce dernier module.\n#!/usr/bin/python # _*_ coding: utf-8 _*_ # segment chinese text # modules import re import sys import jieba # functions def tokenize(file): # input file with open(file, 'r', encoding='utf-8') as f: content = f.","tags":null,"title":"Cours 9 - méthode 2 de de la segmentation du chinois","type":"docs"},{"authors":null,"categories":null,"content":" N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nPour que notre script final soit plus lisible et clair, nous avons essayé de remanier notre code en mettant des blocs de code répétitifs dans une fonction. Grâce aux exemples fournis dans le blog d\u0026rsquo;un ancien camarade, nous avons pu comprendre le foctionnement des fonctions en bash dans un cas précis et concret.\nScript principal #!/bin/bash # importer les fonctions . /Users/becca/Documents/nlp/coursS1/projetEncadre/PROJET-MOT-SUR-LE-WEB/PROGRAMMES/functions.sh #--------------------Main part-----------------# # on commence par supprimer l'éventuel fichier de résultat que l'on doit reconstruire rm -f \u0026quot;$2/tableau2.html\u0026quot; ; # on récupère les 2 arguments que l'on a passé au programme # le premier : chemin vers le dossier contenant les fichiers d'URL # le second : chemin vers le dossier devant contenir le fichier HTML final echo \u0026quot;Les urls SONT DANS : $1\u0026quot; ; echo \u0026quot;On créé le tableau HTML dans : $2\u0026quot; ; # output=$($2/tableau2.html) ecrireMetaData $2/tableau2.html # Création d'une variable pour compter les fichiers traités et donc le nb de tableau généré numerotableau=1; # Création d'une variable pour stocker notre terme ciblé motif=$3; # Parcours du dossier contenant les fichiers URLs for fichier in $(ls $1) do compteur=1; # compteur destiné à compter les URLs pour chaque fichier d'URL echo \u0026quot;$1/$fichier\u0026quot; ; #----------------------------------------------------------- # Création du tableau associé au fichier en cours de traitement #----------------------------------------------------------- ecrireTitre $2/tableau2.html # \u0026quot;parcours\u0026quot; d'un fichier d'URL : lecture ligne à ligne des URLs for ligne in $(cat \u0026quot;$1/$fichier\u0026quot;) do codeRetour=$(detHttpCode $ligne); if [[ $codeRetour == 200 ]] then encodage=$(detEncodage1 $ligne); curl -L -o \u0026quot;../PAGES-ASPIREES/$numerotableau-$compteur.html\u0026quot; \u0026quot;$ligne\u0026quot;; echo \u0026quot;ENCODAGE DETECTE PAR CURL : $encodage\u0026quot;; if [[ $encodage == \u0026quot;UTF-8\u0026quot; ]] then procUtf8 $2/tableau2.html else # http_code=200, l'encodage n'est pas utf-8 encodage=$(detEncodage2 $ligne) if [[ $encodage == \u0026quot;UTF-8\u0026quot; ]] then procUtf8 $2/tableau2.html else code=$(iconv -l|egrep -i $encodage) if [[ $code=true ]] then iconv -f $encodage -t 'utf-8' ../DUMP-TEXT/$numerotableau-$compteur.txt \u0026gt; ../DUMP-TEXT/$numerotableau-$compteur-converti.txt else procVide $2/tableau2.html fi fi fi else procVide $2/tableau2.html fi compteur=$((compteur+1)) ; done echo \u0026quot;\u0026lt;/table\u0026gt;\u0026lt;br /\u0026gt;\u0026quot; \u0026gt;\u0026gt; $2/tableau2.html ; # on incrémente le compteur de tableaux numerotableau=$((numerotableau+1)); done echo \u0026quot;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026quot; \u0026gt;\u0026gt; $2/tableau2.html ;  Fonctions ## functions # 1 html head ecrireMetaData(){ echo \u0026quot;\u0026lt;!DOCTYPE html\u0026gt;\u0026quot; \u0026gt; $1 echo \u0026quot;\u0026lt;html lang=\\\u0026quot;en\\\u0026quot;\u0026gt;\u0026quot; \u0026gt;\u0026gt; $1 echo \u0026quot;\u0026lt;head\u0026gt;\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\u0026lt;title\u0026gt;Projet Encadre\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026quot; \u0026gt;\u0026gt; $1 echo \u0026quot;\u0026lt;body\u0026gt;\u0026quot; \u0026gt;\u0026gt; $1 } # 2 table title ecrireTitre(){ echo \u0026quot;\u0026lt;table border=\\\u0026quot;2\\\u0026quot; align=\\\u0026quot;center\\\u0026quot; width=\\\u0026quot;80%\\\u0026quot;\u0026gt;\u0026quot; \u0026gt;\u0026gt; $1 echo \u0026quot;\u0026lt;tr bgcolor=\\\u0026quot;grey\\\u0026quot;\u0026gt;\u0026lt;td\u0026gt;N°\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;URL\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Code http\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;encodage\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Page aspirée\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Dump\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Filtrage Txt\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Filtrage Html\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Index\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Bitexte\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Fq Motif\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; $1 } # 3 get http_code detHttpCode(){ curl -SIL -o toto -w \u0026quot;%{http_code}\u0026quot; $1 } # 4 two ways to get encoding detEncodage1(){ curl -sIL -o toto -w %{content_type} $1 | cut -f2 -d\u0026quot;=\u0026quot; | tr '[a-z]' '[A-Z]' | tr -d '\\r' } detEncodage2(){ egrep -oi 'charset=\u0026quot;?[^\u0026quot;,]+\u0026quot;?' $1 | cut -f2 -d\u0026quot;=\u0026quot; | tr '[a-z]' '[A-Z]' | tr -d '\\r'| head -1 } # 5 process text when encoding is utf-8 procUtf8(){ # 1. On lynx la page aspirée lynx -dump -nolist -assume_charset=$encodage -display_charset=$encodage \u0026quot;../PAGES-ASPIREES/$numerotableau-$compteur.html\u0026quot; \u0026gt; ../DUMP-TEXT/$numerotableau-$compteur.txt; #----------------------------------------------------------- # 2. On cree le fichier contexte TXT via egrep egrep -i -C2 \u0026quot;$motif\u0026quot; ../DUMP-TEXT/$numerotableau-$compteur.txt \u0026gt; ../CONTEXTE/$numerotableau-$compteur.txt; #----------------------------------------------------------- # 3. Fq motif nbmotif=$(egrep -coi \u0026quot;$motif\u0026quot; ../DUMP-TEXT/$numerotableau-$compteur.txt;); #----------------------------------------------------------- # 4. contexte html ../minigrep/minigrepmultilingue.pl \u0026quot;utf-8\u0026quot; ../DUMP-TEXT/$numerotableau-$compteur.txt ../minigrep/motif-regexp.txt ; mv resultat-extraction.html ../CONTEXTES/$numerotableau-$compteur.html ; #----------------------------------------------------------- # 5. index hierarchique egrep -o \u0026quot;\\w+\u0026quot; ../DUMP-TEXT/$numerotableau-$compteur.txt | sort | uniq -c | sort -r \u0026gt; ../DUMP-TEXT/index-$numerotableau-$compteur.txt ; #----------------------------------------------------------- # 6. bigramme egrep -o \u0026quot;\\w+\u0026quot; ../DUMP-TEXT/$numerotableau-$compteur.txt \u0026gt; bi1.txt; tail -n +2 bi1.txt \u0026gt; bi2.txt ; paste bi1.txt bi2.txt \u0026gt; bi3.txt ; cat bi3.txt | sort | uniq -c | sort -r \u0026gt; ../DUMP-TEXT/bigramme-$numerotableau-$compteur.txt ; #----------------------------------------------------------- # 7. on écrit les résultats dans le tableau avec tous les résultats produits echo \u0026quot;\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;$compteur\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;$ligne\\\u0026quot; target=\\\u0026quot;_blank\\\u0026quot;\u0026gt;$ligne\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Code_http:$codeRetour\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Encodage:$encodage\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../PAGES-ASPIREES/$numerotableau-$compteur.html\\\u0026quot;\u0026gt;$numerotableau-$compteur.html\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../DUMP-TEXT/$numerotableau-$compteur.txt\\\u0026quot;\u0026gt;$numerotableau-$compteur.txt\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../CONTEXTES/$numerotableau-$compteur.txt\\\u0026quot;\u0026gt;$numerotableau-$compteur.txt\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../CONTEXTES/$numerotableau-$compteur.html\\\u0026quot;\u0026gt;$numerotableau-$compteur.html\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../DUMP-TEXT/index-$numerotableau-$compteur.txt\\\u0026quot;\u0026gt;index-$numerotableau-$compteur\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../DUMP-TEXT/bigramme-$numerotableau-$compteur.txt\\\u0026quot;\u0026gt;bigramme-$numerotableau-$compteur\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;$nbmotif\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$1\u0026quot; } # 6 write \u0026quot;-\u0026quot; in table procVide(){ echo \u0026quot;\u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;$compteur\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;$ligne\\\u0026quot; target=\\\u0026quot;_blank\\\u0026quot;\u0026gt;$ligne\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Code_http:$codeRetour\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Encodage:$encodage\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a href=\\\u0026quot;../PAGES-ASPIREES/$numerotableau-$compteur.html\\\u0026quot;\u0026gt;$numerotableau-$compteur.html\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;-\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026quot; \u0026gt;\u0026gt; \u0026quot;$1\u0026quot; }  ","date":1573599600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573599600,"objectID":"5617278c3fe29604092202990dbc88fd","permalink":"/projet_encadre/blog/8/","publishdate":"2019-11-13T00:00:00+01:00","relpermalink":"/projet_encadre/blog/8/","section":"blog","summary":"N.B. Les explications de chaque commande ou de chaque bloc de codes sont données dans les commentaires.\nPour que notre script final soit plus lisible et clair, nous avons essayé de remanier notre code en mettant des blocs de code répétitifs dans une fonction. Grâce aux exemples fournis dans le blog d\u0026rsquo;un ancien camarade, nous avons pu comprendre le foctionnement des fonctions en bash dans un cas précis et concret.","tags":null,"title":"Cours 10 - Reconstruction du code (Refactoring)","type":"docs"}]